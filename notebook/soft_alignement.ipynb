{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pylab import *\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import itertools\n",
    "# import seaborn as sb\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_coords(df, pdb1, pdb2, a, b):\n",
    "    \n",
    "    #df is your previous alignment dataframe \n",
    "    #labels are your pdb ids\n",
    "    \n",
    "    new_coords = []\n",
    "    sequence = []\n",
    "\n",
    "    for iv in df.index.values:\n",
    "\n",
    "        if df.loc[iv][pdb2] == \"-\":\n",
    "            #get \n",
    "#             x = a[df.loc[iv][pdb1].replace(pdb1+\" \", \"\")]\n",
    "            x = a[df.loc[iv][pdb1]]\n",
    "            new_coords.append(x)\n",
    "            sequence.append(df.loc[iv][pdb1])\n",
    "\n",
    "        elif df.loc[iv][pdb1] == \"-\": #NEW SIDE \n",
    "\n",
    "#             y = b[df.loc[iv][pdb2].replace(pdb2+\" \", \"\")]\n",
    "            y = b[df.loc[iv][pdb2]]\n",
    "            new_coords.append(y)\n",
    "            sequence.append(df.loc[iv][pdb2])\n",
    "\n",
    "        else: #if both aligned at this position, average the coordinates\n",
    "            \n",
    "#             x = a[df.loc[iv][pdb1].replace(pdb1+\" \", \"\")]\n",
    "#             y = b[df.loc[iv][pdb2].replace(pdb2+\" \", \"\")]\n",
    "            x = a[df.loc[iv][pdb1]]\n",
    "            y = b[df.loc[iv][pdb2]]\n",
    "            z = list(np.add(x,y)/2)\n",
    "\n",
    "            sequence.append(df.loc[iv][pdb1]+\" \"+df.loc[iv][pdb2])\n",
    "            new_coords.append(z)\n",
    "            \n",
    "    return new_coords, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NW_dist_align(seq1, seq2, DM):\n",
    "\n",
    "    #get the length of each chain\n",
    "    n, m = len(seq1), len(seq2)\n",
    "\n",
    "    #store information \n",
    "    subproblems = [[None for j in range(m+1)] for i in range(n+1)]\n",
    "    parents = [[None for j in range(m+1)] for i in range(n+1)]\n",
    "    s = [[(None,None) for j in range(m+1)] for i in range(n+1)]\n",
    "    length = [[0 for j in range(m+1)] for i in range(n+1)]\n",
    "\n",
    "\n",
    "    #fill in edges\n",
    "\n",
    "    for i in range(n+1):\n",
    "        subproblems[i][0] = i*2.5\n",
    "        parents[i][0] = (i-1,0)\n",
    "        if i != 0:\n",
    "            s[i][0] = (seq1[i-1], \"-\")\n",
    "            length[i][0] = i\n",
    "\n",
    "    for j in range(m+1):\n",
    "        subproblems[0][j] = j*2.5\n",
    "        parents[0][j] = (0,j-1)\n",
    "        if j != 0:\n",
    "            s[0][j] = (\"-\", seq2[j-1])\n",
    "            length[0][j] = j\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,m+1):\n",
    "            \n",
    "            #check the smallest distance so far\n",
    "            #diagonal, down, right\n",
    "\n",
    "            #gap penalty of 2.5\n",
    "            gap = 2.5\n",
    "            x,y = seq1[i-1], seq2[j-1]\n",
    "\n",
    "            options = [(subproblems[i-1][j-1]+DM.loc[x][y], \"diag\"), (subproblems[i][j-1]+gap, \"right\"), (subproblems[i-1][j]+gap, \"down\")]\n",
    "            a,b = sorted(options, key = lambda x: x[0])[0]\n",
    "\n",
    "            if b == \"diag\":\n",
    "                subproblems[i][j] = subproblems[i-1][j-1]+DM.loc[x][y] #+diff\n",
    "                parents[i][j] = (i-1, j-1)\n",
    "                s[i][j] = (seq1[i-1], seq2[j-1])\n",
    "                length[i][j] = length[i-1][j-1]+1\n",
    "\n",
    "            else:\n",
    "                if b == \"down\":\n",
    "                    subproblems[i][j] = subproblems[i-1][j]+gap #+DM.loc[x][y]\n",
    "                    parents[i][j] = (i-1, j)\n",
    "                    s[i][j] = (seq1[i-1],\"-\") #seq1[i-1] \n",
    "                    length[i][j] = length[i-1][j]+1\n",
    "                else:\n",
    "                    subproblems[i][j] = subproblems[i][j-1]+gap #+DM.loc[x][y]\n",
    "                    parents[i][j] = (i, j-1)\n",
    "                    s[i][j] = (\"-\", seq2[j-1]) #seq2[j-1]\n",
    "                    length[i][j] = length[i][j-1]+1\n",
    "\n",
    "    seq = [s[n][m]]\n",
    "    align_1 = [s[n][m][0]]\n",
    "    align_2 = [s[n][m][1]]\n",
    "\n",
    "    x,y = n,m\n",
    "\n",
    "    while len(seq) < length[n][m]:\n",
    "        x,y = parents[x][y]\n",
    "        seq.insert(0, s[x][y])\n",
    "        align_1.insert(0, s[x][y][0])\n",
    "        align_2.insert(0, s[x][y][1])\n",
    "\n",
    "#     return seq\n",
    "#     pdb1 = seq1[0][:4]\n",
    "#     pdb2 = seq2[0][:4]\n",
    "    \n",
    "    calculate_coverage = 0\n",
    "    length = 0\n",
    "    \n",
    "    distances = []\n",
    "    for i in range(len(align_1)):\n",
    "        x,y = align_1[i],align_2[i]\n",
    "#         print x,y\n",
    "        if x != \"-\" and y != \"-\":\n",
    "            distances.append(DM.loc[x][y])\n",
    "        else:\n",
    "            distances.append(\"-\")\n",
    "\n",
    "        #x is the core, want to count how many times matches \n",
    "        if x != \"-\" and y == \"-\":\n",
    "#             print x, y\n",
    "            calculate_coverage += 1\n",
    "        if x != \"-\":\n",
    "            length += 1\n",
    "    \n",
    "#     print subproblems[-1][-1]\n",
    "#     print pdb1, pdb2\n",
    "    coverage = 1-calculate_coverage/float(length)\n",
    "    df = pd.DataFrame({\"1\":align_1, \"2\":align_2, \"distance\":distances})\n",
    "    count = 0\n",
    "    for iv in df.index.values:\n",
    "        if \"-\" not in set(df.loc[iv]):\n",
    "            count+= 1\n",
    "#     print \"coverage:\", coverage\n",
    "    return align_1, align_2, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_progressive_alignments(bindingmodes, pdb_to_paths):\n",
    "\n",
    "    alignments = {}\n",
    "\n",
    "    for bm in bindingmodes: \n",
    "        pdbs_list = bindingmodes[bm]\n",
    "        pdbs = [p for p in pdbs_list]\n",
    "        if len(pdbs) > 1: \n",
    "\n",
    "            #do first pairing \n",
    "            pdb1 = pdbs.pop(0)\n",
    "            pdb2 = pdbs.pop(0)\n",
    "            \n",
    "#             print pdb1, pdb2\n",
    "\n",
    "            align1, align2, df, new_coords, new_labels, new_dict = getAlignment(pdb1, pdb2, pdb_to_paths)\n",
    "            \n",
    "#             print df\n",
    "\n",
    "            while len(pdbs) > 0: \n",
    "                \n",
    "#                 pdb= path.split(\"/\")[-1][:4]\n",
    "#                 chain = path.split(\"/\")[-1][5]\n",
    "#                 pdb_chain = path.split(\"/\")[-1][:6]\n",
    "                \n",
    "                \n",
    "                pdb3 = pdbs.pop(0)\n",
    "                path3 = pdb_to_paths[pdb3]\n",
    "                chain = path3.split(\"/\")[-1][12]\n",
    "                \n",
    "#                 print pdb3, chain, path3\n",
    "\n",
    "                labels3, coords3, coordDict3 = getCoordDictionarys(path3, pdb3[:4], chain)\n",
    "\n",
    "                align1, align2, df, new_coords, new_labels, new_dict = getAlignment2(new_labels, new_coords, new_dict, labels3, coords3, coordDict3)\n",
    "\n",
    "            alignments[bm] = df\n",
    "            \n",
    "    return alignments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAlignment2(labels1, coords1, coordDict1, labels2, coords2, coordDict2):\n",
    "\n",
    "    #step 1: form a new distance matrix \n",
    "    DM = makeDistanceMatrix(coords1, coords2, labels1, labels2)\n",
    "    \n",
    "    #step 2: get the new alignment \n",
    "    align1, align2, df = NW_dist_align(labels1, labels2, DM)\n",
    "    \n",
    "    #step 3: get the average coordinates from this df\n",
    "    new_coords, new_labels = combine_coords(df, list(df.columns)[0], list(df.columns)[1], coordDict1, coordDict2)\n",
    "    \n",
    "    #new_dict = {new_labels[i]: new_coords[i] for i in range(len(new_coords))}\n",
    "    new_dict = {}\n",
    "    for i in range(len(new_coords)):\n",
    "        new_dict[new_labels[i]] = new_coords[i]\n",
    "    return align1, align2, df, new_coords, new_labels, new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAlignment(path1, pdb1, chain1,path2, pdb2, chain2):\n",
    "    \n",
    "\n",
    "    \n",
    "    labels1, coords1, coordDict1 = getCoordDictionarys(path1, pdb1, chain1)\n",
    "    labels2, coords2, coordDict2 = getCoordDictionarys(path2, pdb2, chain2)\n",
    "    \n",
    "    #step 1: form a new distance matrix\n",
    "    \n",
    "    DM = makeDistanceMatrix(coords1, coords2, labels1, labels2)\n",
    "    \n",
    "    #step 2: get the new alignment \n",
    "    align1, align2, df = NW_dist_align(labels1, labels2, DM)\n",
    "    \n",
    "    #step 3: get the average coordinates from this df\n",
    "    new_coords, new_labels = combine_coords(df, list(df.columns)[0], list(df.columns)[1], coordDict1, coordDict2)\n",
    "    \n",
    "    #new_dict = {new_labels[i]:new_coords[i] for i in range(len(new_coords))}\n",
    "    #new_dict = {new_labels[i]: new_coords[i] for i in range(len(new_coords))}\n",
    "    new_dict = {}\n",
    "    for i in range(len(new_coords)):\n",
    "        new_dict[new_labels[i]] = new_coords[i]\n",
    "\n",
    "    return align1, align2, df, new_coords, new_labels, new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCoordDictionarys(pdb_path, pdb, chain):\n",
    "\n",
    "    ArrCord = getCalphasChain(pdb_path, chain, pdb)   \n",
    "    \n",
    "    labels = []\n",
    "    coords = []\n",
    "    \n",
    "    #make sure sorted\n",
    "    coordDict = dict()\n",
    "    for k in ArrCord:\n",
    "        labels.append(k[3])\n",
    "        coords.append(k[0:3])\n",
    "        coordDict[k[3]] = k[0:3]\n",
    "    return labels, coords, coordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For a pdb file, returns a dictionary of the residues specific chain    \n",
    "def getCalphasChain(filename, chain, pdb):\n",
    "    f = open(filename, 'r')\n",
    "    #print(filename)\n",
    "    \n",
    "    m = re.search(\"(....)_(\\S+)_(\\S+)_(\\d+).pdb\",filename.split(\"/\")[-1])\n",
    "    fpdb= m.group(1)\n",
    "    fchain = m.group(3)\n",
    "    frchain = m.group(2)\n",
    "    \n",
    "    lines = []\n",
    "    for line in f:\n",
    "        if ' CA ' in line:\n",
    "            if \" \"+chain+\" \" in line:\n",
    "                lines.append(line)\n",
    "    CAdict = []\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        resNum = lines[i].split()[5]\n",
    "        chain = lines[i].split()[4]\n",
    "        res = lines[i].split()[3]\n",
    "        resNum = resNum+chain+\"_\"+fpdb+\"_\"+frchain+\"_\"+fchain+\"_\"+res\n",
    "        xCoord = float(lines[i][30:38].replace(\" \",\"\"))\n",
    "        yCoord = float(lines[i][39:46].replace(\" \",\"\"))\n",
    "        zCoord = float(lines[i][46:56].replace(\" \",\"\"))\n",
    "    \n",
    "        coordinates = [xCoord,yCoord,zCoord,resNum]\n",
    "        CAdict.append(coordinates)\n",
    "    return CAdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeDistanceMatrix(coords_1, coords_2, labels1, labels2):\n",
    "    #print(coords_1, coords_2)\n",
    "    dist_matrix = euclidean_distances(coords_1, coords_2)\n",
    "    DM = pd.DataFrame(dist_matrix, index=labels1, columns=labels2)\n",
    "    \n",
    "    return DM\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"/media/vince/Postdoc/PixelDB/PixelDB/other_files/all_pairwise_TM.dat\")\n",
    "content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AllIden = dict()\n",
    "for l in content:\n",
    "    sp = l.split(\" \")\n",
    "    #print(sp)\n",
    "    if (len(sp) != 6):\n",
    "        continue\n",
    "    m = re.search(\"ALI:(\\d+)\",sp[3])\n",
    "    match = int(m.group(1))\n",
    "    \n",
    "    m = re.search(\"IDEN:(\\d+)\",sp[2])\n",
    "    iden = int(m.group(1))\n",
    "    \n",
    "    m = re.search(\"TOT1:(\\d+)\",sp[4])\n",
    "    len1 = int(m.group(1))\n",
    "    \n",
    "    m = re.search(\"TOT2:(\\d+)\",sp[5])\n",
    "    len2 = int(m.group(1))\n",
    "    \n",
    "        \n",
    "    \n",
    "    maxl = np.max([len1,len2])\n",
    "    if maxl == 0:\n",
    "        maxl = 10\n",
    "        match = 0\n",
    "        iden = 0\n",
    "    TM = float(iden) / maxl\n",
    "    #if (TM < 0.8):\n",
    "    #    continue\n",
    "    \n",
    "    #if len(OriTen) > 100:\n",
    "    #    if sp[0] not in OriTen:\n",
    "    #        continue\n",
    "    #    if sp[1] not in OriTen:\n",
    "    #        continue\n",
    "    #if sp[0] not in OriTen:\n",
    "    #    OriTen.append(sp[0])\n",
    "    #if sp[1] not in OriTen:\n",
    "    #   OriTen.append(sp[1])\n",
    "    #print(TM,sp)\n",
    "    for i in range(0,2):\n",
    "        if sp[i] not in AllIden:\n",
    "            AllIden[sp[i]] = dict()\n",
    "            AllIden[sp[i]][sp[i]] = 1.0\n",
    "        if sp[0] == sp[1]:\n",
    "            AllIden[sp[0]][sp[1]] = 1.0\n",
    "            AllIden[sp[1]][sp[0]] = 1.0\n",
    "        else:\n",
    "            if i == 0:\n",
    "                if sp[1] in AllIden[sp[i]]:\n",
    "                    print(sp)\n",
    "                    die\n",
    "                AllIden[sp[i]][sp[1]] = TM\n",
    "            else:\n",
    "                AllIden[sp[1]][sp[0]] = TM\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-27635892959d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mallfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/scratch/users/madduran/PixelDB/aln_pdb/*.pdb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mallfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallfile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "allfile = glob.glob('/scratch/users/madduran/PixelDB/aln_pdb/*.pdb') \n",
    "allfile = allfile[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Same = dict()\n",
    "for i in range(0,len(allfile)):\n",
    "    m = re.search(\"(....)_(\\S+)_(\\S+)_(\\d+).pdb\",allfile[i].split(\"/\")[-1])\n",
    "    pdb1= m.group(1)\n",
    "    chain1 = m.group(3)\n",
    "    rchain1 = m.group(2)\n",
    "    labels1, coords1, coordDict1 = getCoordDictionarys(allfile[i], pdb1, chain1)\n",
    "    n1 = pdb1+\"_\"+rchain1\n",
    "    for j in range(i+1,len(allfile)):\n",
    "        m = re.search(\"(....)_(\\S+)_(\\S+)_(\\d+).pdb\",allfile[j].split(\"/\")[-1])\n",
    "        pdb2= m.group(1)\n",
    "        chain2 = m.group(3)\n",
    "        rchain2 = m.group(2)\n",
    "        labels2, coords2, coordDict2 = getCoordDictionarys(allfile[j], pdb2, chain2)\n",
    "        n2 = pdb2+\"_\"+rchain2\n",
    "        DM = makeDistanceMatrix(coords1, coords2, labels1, labels2)\n",
    "        \n",
    "        align1, align2, df = NW_dist_align(labels1,labels2, DM)\n",
    "        Iden = 0.0\n",
    "        Tot = 0.0\n",
    "        #print(df)\n",
    "        for (aa,bb) in zip(align1,align2):\n",
    "            Tot += 1\n",
    "            if (aa != \"-\") and (bb != \"-\"):\n",
    "                if str(aa[-3:]) == str(bb[-3:]):\n",
    "                    #print(aa,bb)\n",
    "                    Iden += 1.0\n",
    "            \n",
    "        tn1 = allfile[j]\n",
    "        tn2 = allfile[i]\n",
    "        if tn1 not in Same:\n",
    "            Same[tn1] = dict()\n",
    "        if tn2 not in Same:\n",
    "            Same[tn2] = dict()\n",
    "        if (AllIden[n1][n2] > 0.95) and (Iden/Tot > 0.99):\n",
    "            Same[tn1][tn2] = 1\n",
    "            Same[tn2][tn1] = 1\n",
    "            #print(\"THIS = \",n1,chain1,n2,chain2)\n",
    "        else:\n",
    "            Same[tn1][tn2] = 0\n",
    "            Same[tn2][tn1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/4KTC_C_D_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/4JMY_B_D_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/4I32_B_D_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/1DXP_A_C_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/4I31_B_D_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/2A4G_A_B_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/1DY8_A_C_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/3KN2_A_B_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/3P8O_B_D_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/3KEE_C_G_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/1DY9_A_C_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/1JXP_A_C_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/2A4R_A_B_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/2OIN_B_D_14.pdb']\n",
      "['/media/vince/Postdoc/PixelDB/PixelDB/wrk/4I33_B_D_14.pdb']\n"
     ]
    }
   ],
   "source": [
    "tdf = pd.DataFrame(Same).fillna(0)\n",
    "#print(tdf)\n",
    "tdfname = list(tdf.columns.values)\n",
    "for i in range(0,len(tdf)):\n",
    "    samec = tdf.sum()\n",
    "    Ind = np.argsort(samec)\n",
    "    if samec[Ind[-1]] == 0:\n",
    "        break\n",
    "    #print(dfname[Ind[-1]])\n",
    "    print(list(np.array(tdfname)[np.array(tdfname) == tdfname[Ind[-1]]]))\n",
    "    \n",
    "    tdf = tdf[list(np.array(tdfname)[np.array(tdfname) != tdfname[Ind[-1]]])].transpose()\n",
    "    tdf = tdf[list(np.array(tdfname)[np.array(tdfname) != tdfname[Ind[-1]]])].transpose()\n",
    "    tdfname = list(tdf.columns.values)\n",
    "    #print(tdfname[Ind[-1]])\n",
    "    #print(tdf[tdfname[Ind[-1]]])\n",
    "    #die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Consec = 4\n",
    "allfileNR = list(tdf.columns.values)\n",
    "G=nx.Graph()\n",
    "for i in range(0,len(allfileNR)):\n",
    "    #Get some label\n",
    "    m = re.search(\"(....)_(\\S+)_(\\S+)_(\\d+).pdb\",allfileNR[i].split(\"/\")[-1])\n",
    "    pdb1= m.group(1)\n",
    "    chain1 = m.group(3)\n",
    "    rchain1 = m.group(2)\n",
    "    labels1, coords1, coordDict1 = getCoordDictionarys(allfileNR[i], pdb1, chain1)\n",
    "    n1 = pdb1+\"_\"+rchain1\n",
    "    for j in range(i+1,len(allfileNR)):\n",
    "        #Get some Label\n",
    "        m = re.search(\"(....)_(\\S+)_(\\S+)_(\\d+).pdb\",allfileNR[j].split(\"/\")[-1])\n",
    "        pdb2= m.group(1)\n",
    "        chain2 = m.group(3)\n",
    "        rchain2 = m.group(2)\n",
    "        labels2, coords2, coordDict2 = getCoordDictionarys(allfileNR[j], pdb2, chain2)\n",
    "        n2 = pdb2+\"_\"+rchain2\n",
    "        \n",
    "        #Get Distance matrix and align\n",
    "        DM = makeDistanceMatrix(coords1, coords2, labels1, labels2)\n",
    "        align1, align2, df = NW_dist_align(labels1,labels2, DM)\n",
    "        for k in range(0,len(align1)-Consec):\n",
    "            suba1 = align1[k:k+Consec]\n",
    "            suba2 = align2[k:k+Consec]\n",
    "            if (\"-\" in suba1):\n",
    "                continue\n",
    "            if (\"-\" in suba2):\n",
    "                continue\n",
    "            #print(k,\" \".join(suba1),\" \".join(suba2))\n",
    "            G.add_edge(\" \".join(suba1),\" \".join(suba2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%pylab inline\n",
    "#%matplotlib inline\n",
    "#nx.draw(G)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Gi = G.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3KEE_A_E', '2QV1_B_D', '1JXP_B_D', '1W3C_B_D', '2GVF_A_B', '2O8M_A_C', '3P8N_B_D']\n",
      "['3RC5_A_B', '4A1T_B_D', '3RC4_A_B', '4A1V_A_C', '3M5O_B_D', '3M5N_D_H', '4JMY_B_F']\n",
      "['4A1X_B_D']\n"
     ]
    }
   ],
   "source": [
    "BindindMode = []\n",
    "AllAdded = []\n",
    "G = Gi.copy()\n",
    "for i in range(0,len(G.nodes())):\n",
    "    MaxClique = []\n",
    "    MaxSize = 0\n",
    "    for cl in list(nx.find_cliques(G)):\n",
    "        if len(cl) > MaxSize:\n",
    "            MaxSize = len(cl)\n",
    "            MaxClique = cl\n",
    "    bm = []\n",
    "    #print(MaxClique)\n",
    "    for it in MaxClique:\n",
    "        #Find PDB\n",
    "        tb = \"_\".join(re.split(\"_\",it)[1:4])\n",
    "        #print(tb)\n",
    "        bm.append(tb)\n",
    "        AllAdded.append(tb)\n",
    "    print(bm)\n",
    "    BindindMode.append(bm)\n",
    "    for n in G.nodes():\n",
    "        for tb in AllAdded:\n",
    "            if re.search(tb,n):\n",
    "                G.remove_node(n)\n",
    "    if (len(G.nodes()) == 0):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BmToFile = dict()\n",
    "for bm in BindindMode:\n",
    "    #print(bm)\n",
    "    for b in bm:\n",
    "        for f in allfileNR:\n",
    "            if re.search(b,f):\n",
    "                #print(b,f)\n",
    "                BmToFile[b] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Binding mode=', ['3KEE_A_E', '2QV1_B_D', '1JXP_B_D', '1W3C_B_D', '2GVF_A_B', '2O8M_A_C', '3P8N_B_D'])\n",
      "['1E_3KEE_A_E_LYS', '1D_2QV1_B_D_LYS', '1B_2GVF_A_B_LYS', '1C_2O8M_A_C_LYS', '1D_3P8N_B_D_LYS', 0.7142857142857143]\n",
      "['2E_3KEE_A_E_GLY', '2D_2QV1_B_D_GLY', '1D_1JXP_B_D_GLY', '1D_1W3C_B_D_GLY', '2B_2GVF_A_B_GLY', '2C_2O8M_A_C_GLY', '2D_3P8N_B_D_GLY', 1.0]\n",
      "['3E_3KEE_A_E_SER', '3D_2QV1_B_D_SER', '2D_1JXP_B_D_SER', '2D_1W3C_B_D_SER', '3B_2GVF_A_B_SER', '3C_2O8M_A_C_CYS', '3D_3P8N_B_D_SER', 1.0]\n",
      "['4E_3KEE_A_E_VAL', '4D_2QV1_B_D_VAL', '3D_1JXP_B_D_VAL', '3D_1W3C_B_D_VAL', '4B_2GVF_A_B_VAL', '4C_2O8M_A_C_VAL', '4D_3P8N_B_D_VAL', 1.0]\n",
      "['5E_3KEE_A_E_VAL', '5D_2QV1_B_D_VAL', '4D_1JXP_B_D_VAL', '4D_1W3C_B_D_VAL', '5B_2GVF_A_B_VAL', '5C_2O8M_A_C_VAL', '5D_3P8N_B_D_VAL', 1.0]\n",
      "['6E_3KEE_A_E_ILE', '6D_2QV1_B_D_ILE', '5D_1JXP_B_D_ILE', '5D_1W3C_B_D_ILE', '6B_2GVF_A_B_ILE', '6C_2O8M_A_C_ILE', '6D_3P8N_B_D_ILE', 1.0]\n",
      "['7E_3KEE_A_E_VAL', '7D_2QV1_B_D_VAL', '6D_1JXP_B_D_VAL', '6D_1W3C_B_D_VAL', '7B_2GVF_A_B_VAL', '7C_2O8M_A_C_VAL', '7D_3P8N_B_D_VAL', 1.0]\n",
      "['8E_3KEE_A_E_GLY', '8D_2QV1_B_D_GLY', '7D_1JXP_B_D_GLY', '7D_1W3C_B_D_GLY', '8B_2GVF_A_B_GLY', '8C_2O8M_A_C_GLY', '8D_3P8N_B_D_GLY', 1.0]\n",
      "['9E_3KEE_A_E_ARG', '9D_2QV1_B_D_ARG', '8D_1JXP_B_D_ARG', '8D_1W3C_B_D_ARG', '9B_2GVF_A_B_ARG', '9C_2O8M_A_C_ARG', '9D_3P8N_B_D_ARG', 1.0]\n",
      "['10E_3KEE_A_E_ILE', '10D_2QV1_B_D_ILE', '9D_1JXP_B_D_ILE', '9D_1W3C_B_D_ILE', '10B_2GVF_A_B_ILE', '10C_2O8M_A_C_ILE', '10D_3P8N_B_D_ILE', 1.0]\n",
      "['11E_3KEE_A_E_VAL', '11D_2QV1_B_D_VAL', '10D_1JXP_B_D_ILE', '10D_1W3C_B_D_ILE', '11B_2GVF_A_B_VAL', '11C_2O8M_A_C_VAL', '11D_3P8N_B_D_ILE', 1.0]\n",
      "['12E_3KEE_A_E_LEU', '12D_2QV1_B_D_LEU', '11D_1JXP_B_D_LEU', '11D_1W3C_B_D_LEU', '12B_2GVF_A_B_LEU', '12C_2O8M_A_C_LEU', '12D_3P8N_B_D_LEU', 1.0]\n",
      "['13D_2QV1_B_D_SER', '12D_1JXP_B_D_SER', '12D_1W3C_B_D_SER', '13B_2GVF_A_B_SER', '13C_2O8M_A_C_SER', '13D_3P8N_B_D_SER', 0.8571428571428571]\n",
      "['14D_2QV1_B_D_GLY', '14B_2GVF_A_B_GLY', '14C_2O8M_A_C_GLY', 0.42857142857142855]\n",
      "['15D_2QV1_B_D_LYS', '15B_2GVF_A_B_LYS', '15C_2O8M_A_C_LYS', 0.42857142857142855]\n",
      "['16D_2QV1_B_D_PRO', '16B_2GVF_A_B_PRO', '16C_2O8M_A_C_PRO', 0.42857142857142855]\n",
      "['17D_2QV1_B_D_ALA', '17B_2GVF_A_B_ALA', '17C_2O8M_A_C_ALA', 0.42857142857142855]\n",
      "['18D_2QV1_B_D_ILE', '18B_2GVF_A_B_ILE', '18C_2O8M_A_C_ILE', 0.42857142857142855]\n",
      "['19D_2QV1_B_D_ILE', '19B_2GVF_A_B_ILE', '19C_2O8M_A_C_ILE', 0.42857142857142855]\n",
      "['20D_2QV1_B_D_PRO', '20B_2GVF_A_B_PRO', '20C_2O8M_A_C_PRO', 0.42857142857142855]\n",
      "['21D_2QV1_B_D_ALA', '21B_2GVF_A_B_LYS', '21C_2O8M_A_C_LYS', 0.42857142857142855]\n",
      "['22B_2GVF_A_B_LYS', '22C_2O8M_A_C_LYS', 0.2857142857142857]\n",
      "('Binding mode=', ['3RC5_A_B', '4A1T_B_D', '3RC4_A_B', '4A1V_A_C', '3M5O_B_D', '3M5N_D_H', '4JMY_B_F'])\n",
      "['1B_3RC5_A_B_GLN', '1D_3M5O_B_D_THR', '1H_3M5N_D_H_SER', 0.42857142857142855]\n",
      "['2B_3RC5_A_B_GLU', '1D_4A1T_B_D_GLY', '1B_3RC4_A_B_PRO', '1C_4A1V_A_C_ASP', '2D_3M5O_B_D_GLU', '2H_3M5N_D_H_GLU', '1F_4JMY_B_F_ASP', 1.0]\n",
      "['3B_3RC5_A_B_ARG', '2D_4A1T_B_D_ARG', '2B_3RC4_A_B_SER', '2C_4A1V_A_C_GLU', '3D_3M5O_B_D_ASP', '3H_3M5N_D_H_CYS', '2F_4JMY_B_F_ASP', 1.0]\n",
      "['4B_3RC5_A_B_GLU', '3D_4A1T_B_D_LEU', '3B_3RC4_A_B_SER', '3C_4A1V_A_C_LEU', '4D_3M5O_B_D_VAL', '4H_3M5N_D_H_THR', '3F_4JMY_B_F_ILE', 1.0]\n",
      "['5B_3RC5_A_B_VAL', '4D_4A1T_B_D_VAL', '4B_3RC4_A_B_THR', '4C_4A1V_A_C_VAL', '5D_3M5O_B_D_VAL', '5H_3M5N_D_H_THR', '4F_4JMY_B_F_VAL', 1.0]\n",
      "['6B_3RC5_A_B_PRO', '5D_4A1T_B_D_TYR', '5B_3RC4_A_B_PRO', '5C_4A1V_A_C_TYR', '6D_3M5O_B_D_CYS', '6H_3M5N_D_H_PRO', '5F_4JMY_B_F_PRO', 1.0]\n",
      "['7B_3RC5_A_B_CYS', '6D_4A1T_B_D_LEU', '6B_3RC4_A_B_CYS', '6C_4A1V_A_C_LEU', '7D_3M5O_B_D_CYS', '7H_3M5N_D_H_CYS', '6F_4JMY_B_F_CYS', 1.0]\n",
      "['7D_4A1T_B_D_LEU', '7C_4A1V_A_C_LEU', 0.2857142857142857]\n",
      "['8D_4A1T_B_D_ASP', '8C_4A1V_A_C_ASP', 0.2857142857142857]\n",
      "['9D_4A1T_B_D_GLY', '9C_4A1V_A_C_GLY', 0.2857142857142857]\n",
      "['10D_4A1T_B_D_PRO', '10C_4A1V_A_C_PRO', 0.2857142857142857]\n",
      "['11D_4A1T_B_D_GLY', '11C_4A1V_A_C_GLY', 0.2857142857142857]\n",
      "['12D_4A1T_B_D_TYR', '12C_4A1V_A_C_TYR', 0.2857142857142857]\n",
      "['13D_4A1T_B_D_ASP', '13C_4A1V_A_C_ASP', 0.2857142857142857]\n",
      "['14D_4A1T_B_D_PRO', '14C_4A1V_A_C_PRO', 0.2857142857142857]\n",
      "['15D_4A1T_B_D_ILE', '15C_4A1V_A_C_ILE', 0.2857142857142857]\n",
      "['16D_4A1T_B_D_HIS', '16C_4A1V_A_C_HIS', 0.2857142857142857]\n",
      "['17D_4A1T_B_D_CYS', 0.14285714285714285]\n",
      "['18D_4A1T_B_D_ASP', 0.14285714285714285]\n",
      "('Binding mode=', ['4A1X_B_D'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for bm in BindindMode:\n",
    "    allfile = []\n",
    "    print(\"Binding mode=\",bm)\n",
    "    for bt in bm:\n",
    "        allfile.append(BmToFile[bt])\n",
    "    if (len(allfile) == 1):\n",
    "        continue\n",
    "    #do first pairing \n",
    "    i = 0\n",
    "    j = 1\n",
    "\n",
    "    m = re.search(\"(....)_(\\S+)_(\\S+)_(\\d+).pdb\",allfile[i].split(\"/\")[-1])\n",
    "    pdb1= m.group(1)\n",
    "    chain1 = m.group(3)\n",
    "    rchain1 = m.group(2)\n",
    "    labels1, coords1, coordDict1 = getCoordDictionarys(allfile[i], pdb1, chain1)\n",
    "\n",
    "    m = re.search(\"(....)_(\\S+)_(\\S+)_(\\d+).pdb\",allfile[j].split(\"/\")[-1])\n",
    "    pdb2= m.group(1)\n",
    "    chain2 = m.group(3)\n",
    "    rchain2 = m.group(2)\n",
    "    labels2, coords2, coordDict2 = getCoordDictionarys(allfile[j], pdb2, chain2)\n",
    "\n",
    "\n",
    "    align1, align2, df, new_coords, new_labels, new_dict = getAlignment(allfile[i], pdb1, chain1,allfile[j], pdb2, chain2)\n",
    "    \n",
    "    for j in range(2,len(allfile)):\n",
    "        #print(j)\n",
    "        m = re.search(\"(....)_(\\S+)_(\\S+)_(\\d+).pdb\",allfile[j].split(\"/\")[-1])\n",
    "        pdb2= m.group(1)\n",
    "        chain2 = m.group(3)\n",
    "        rchain2 = m.group(2)\n",
    "        #print(pdb2,chain2,allfile[j])\n",
    "        labels2, coords2, coordDict2 = getCoordDictionarys(allfile[j], pdb2, chain2)\n",
    "\n",
    "        align1, align2, df, new_coords, new_labels, new_dict = getAlignment2(new_labels, new_coords, new_dict, labels2, coords2, coordDict2)\n",
    "        #print(df)\n",
    "        #align1, align2, df, new_coords, new_labels, new_dict = getAlignment(allfile[i], pdb1, chain1,allfile[j], pdb2, chain2)\n",
    "    for i in df[\"1\"]+\" \"+df[\"2\"]:\n",
    "        sp = re.split(\"\\s+\",i)\n",
    "        if sp[-1] == \"-\":\n",
    "            sp = sp[:-1]\n",
    "        conser = float(len(sp))/float(len(bm))\n",
    "        print(sp+[conser])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
